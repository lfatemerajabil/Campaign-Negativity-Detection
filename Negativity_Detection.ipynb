{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83a56ec",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io, json\n",
    "import csv\n",
    "import requests\n",
    "import stanza\n",
    "import emojis\n",
    "import collections\n",
    "import math\n",
    "import twitter\n",
    "import fnmatch\n",
    "import pickle\n",
    "import urllib\n",
    "import re, string, ast, emoji, json, urlexpander, sys\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files  \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import bigrams\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.corpus import words\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from scipy.stats import kurtosis, skew, entropy\n",
    "from textstat.textstat import textstat\n",
    "from textblob import TextBlob\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from persiantools.jdatetime import JalaliDate\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import hazm\n",
    "from hazm import Normalizer, Lemmatizer, WordTokenizer\n",
    "from cleantext import clean\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import imblearn\n",
    "from numpy import mean\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from googletrans import Translator\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "#fasttext.util.download_model('fa', if_exists='ignore')\n",
    "#!pip install googletrans==4.0.0-rc1\n",
    "ft = fasttext.load_model('cc.fa.300.bin')\n",
    "ft.get_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b07b2",
   "metadata": {},
   "source": [
    "# Loading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3c114c",
   "metadata": {},
  },
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac0739",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "organizations = pd.read_excel(r'../Files/organizations.xlsx')\n",
    "organizations['name2'] = organizations['name2'].astype(str)\n",
    "name_of_organizations = organizations['name2'].iloc[:130].to_list()\n",
    "organization = []\n",
    "for n in name_of_organizations:\n",
    "    organization.append(n.rstrip())\n",
    "organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c0e81",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swears = pd.read_json(r'../Files/data_swear.json', encoding='utf-8')\n",
    "swear = swears['word'].to_list()\n",
    "swear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830b73",
   "metadata": {},
   "source": [
    "# Tweet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet_like'] = [like for like in data['favorite_count']]\n",
    "\n",
    "data['tweet_retweet_count'] = [retweet for retweet in data['retweet_count']]\n",
    "\n",
    "data['tweet_length_word'] = [len(word_tokenize(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_length_characters'] = [len(t) for t in data['full_text']]\n",
    "\n",
    "data['hashtags'] = data['full_text'].str.findall(\"(#[^#\\s]+)\")\n",
    "\n",
    "pattern = r'(https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}[-a-zA-Z0-9()@:%_+.~#?&/=]*)'\n",
    "data['urls']= data['full_text'].str.findall(pattern)\n",
    "\n",
    "data['tweet_num_hashtags'] = [len(h) for h in data['hashtags']]\n",
    "\n",
    "def extract_mention_set(text):\n",
    "    mention_list = re.findall(\"@([a-zA-Z0-9]{1,15})\", text)\n",
    "    return len(mention_list)\n",
    "\n",
    "data['tweet_num_mention'] = data['full_text'].apply(extract_mention_set)\n",
    "\n",
    "data['tweet_num_urls'] = [len(h) for h in data['urls']]\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "data['tweet_num_emoji'] = [len(extract_emojis(t)) for t in data['full_text']]\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]\n",
    "\n",
    "persian_punctuation = '.،؛:()«»؟![]-/'\n",
    "def extract_punctuation(s):\n",
    "  return ''.join(c for c in s if c in list(persian_punctuation))\n",
    "\n",
    "data['tweet_num_punctuation'] = [len(extract_punctuation(t)) for t in data['full_text']]\n",
    "\n",
    "list_o = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_o.append(0)\n",
    "    for name in organization:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_o[i] += 1\n",
    "data['organize_names'] = list_o\n",
    "\n",
    "list_sw = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_sw.append(0)\n",
    "    for name in swear:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_sw[i] += 1\n",
    "data['swear_words'] = list_sw\n",
    "\n",
    "cn = ['رئیسی' , 'جلیلی' , 'رییسی' , 'همتی' , 'مهرعلیزاده' , 'مهر علیزاده' , 'محسن رضایی' , 'قاضی زاده' , 'قاضی‌زاده',\n",
    "      'زاکانی' , 'وهاب‌زاده' , 'وهاب زاده' , 'بطحایی' , 'حناچی' , 'مولاوردی' , 'تاج زاده' , 'تاج‌زاده' ,\n",
    "      'تاجزاده' , 'قالیباف' , 'قالی‌باف' , 'قالی باف' , 'محمود صادقی' , 'مهدی اسماعیلی' ,\n",
    "      'محمد اسماعیلی' , 'کواکبیان' , 'جواد اوجی' , 'خاتمی' , 'خامنه‌ای' , 'خامنه ای' , 'محمد حسینی', \n",
    "      'حمید سجادی' , 'معصومه ابتکار' , 'ساداتی نژاد' , 'ساداتی‌نژاد' , 'محمد دهقان' , 'جهرمی' , 'عبدالملکی' , 'لاریجانی',\n",
    "      'عراقچی' , 'ضرغامی' , 'روحانی' , 'رستم قاسمی' , 'مرتضوی' , 'نوبخت' , 'میرکاظمی' , 'مخبر' , 'ظریف' , \n",
    "      'عراق‌چی' , 'عراق چی' , 'زارع پور' , 'زارع‌پور' , 'عین اللهی' , 'عین‌اللهی' , 'جهانگیری' , 'جهان‌گیری' ,\n",
    "      'جهان گیری' , 'خزعلی' , 'اژه‌ای' , 'واعظی' , 'باقری کنی' , 'عبداللهیان' , 'احمدی‌نژاد' , 'احمدی نژاد' , 'آشتیانی' ,\n",
    "     'آقا' , 'آقای' , 'خانم' , 'مسئول' , 'وزیر' , 'آخوند' , 'رهبر' , 'رهبری' , 'آخوند' , 'استاندار' , 'شهردار']\n",
    "list_cn = []\n",
    "for i in range(len(data['full_text'].to_list())):\n",
    "    list_cn.append(0)\n",
    "    for name in cn:\n",
    "        if name in (data['full_text'].to_list())[i]:\n",
    "            list_cn[i] += 1\n",
    "data['person_names'] = list_cn\n",
    "\n",
    "'''def get_domain_from_url(url):\n",
    "  return urllib.parse.urlparse(url).netloc\n",
    "\n",
    "data['urls_dom'] = data['urls'].apply(lambda X: [get_domain_from_url(x) for x in X])\n",
    "url_list= []\n",
    "for i in range(len(data['urls_dom'])):\n",
    "    for url in data['urls_dom'].iloc[i]:\n",
    "        url_list.append(url)\n",
    "url_set = set(url_list)\n",
    "urls = []\n",
    "for url in url_set:\n",
    "    urls.append(url)\n",
    "if(len(urls) >= 10):\n",
    "    U = urls[:10]\n",
    "else:\n",
    "    U = urls\n",
    "for j in range(len(U)):\n",
    "    list_u = [0 for i in range(len(data))]\n",
    "    for i in range(len(data)):\n",
    "        if data['tweet_num_urls'].iloc[i] > 0:\n",
    "            if U[j] in data['full_text'].iloc[i]:\n",
    "                list_u[i] = 1\n",
    "    data[U[j]] = list_u.copy()'''\n",
    "\n",
    "data['tweet_emoji'] = [extract_emojis(t) for t in data['full_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2e5fb",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoji_to_text(text, delimiters=(' ', ' ')):\n",
    "    \"\"\" Convert emojis to something readable by the vocab and model \"\"\"\n",
    "    text = emoji.demojize(text, delimiters=delimiters)\n",
    "    return text\n",
    "emo_list = []\n",
    "emoji_set = set(data[data['tweet_num_emoji']>0]['tweet_emoji'])\n",
    "for e in emoji_set:\n",
    "    for ee in e.strip():\n",
    "        emo_list.append(ee)\n",
    "emo_set = set(emo_list)\n",
    "emo_pic = []\n",
    "for e in emo_set:\n",
    "    emo_pic.append(e)\n",
    "emo = [convert_emoji_to_text(e) for e in emo_set]\n",
    "emo = [e.split('_') for e in emo]\n",
    "emo = [' '.join(e) for e in emo]\n",
    "emo_fa = []\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "for i in range(len(emo)):\n",
    "    trans = translator.translate(emo[i], dest='fa')\n",
    "    emo_fa.append(trans)\n",
    "emo_farsi = []\n",
    "for e in emo_fa:\n",
    "    emo_farsi.append(e.text)\n",
    "emo_farsi = [' ' + e + ' ' for e in emo_farsi]\n",
    "Emojis = dict(zip(emo_pic, emo_farsi))\n",
    "Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffe3565e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_emoji'].iloc[i] > 0:\n",
    "        for e in emo_pic:\n",
    "            if e in data['full_text'].iloc[i]:\n",
    "                data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(e, Emojis[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6e5132ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' اقتصاد اسلامی ',\n",
       " ' عبرت اوکراین ',\n",
       " ' تورم ',\n",
       " ' اندیکا ',\n",
       " ' سوریه نمیشویم ',\n",
       " ' حضرت محمد ',\n",
       " ' مادران تحول ساز ',\n",
       " ' عربستان ',\n",
       " ' سیدمحمدخاتمی: ',\n",
       " ' مافیای واردات ',\n",
       " ' دانشگاه علوم و تحقیقات ',\n",
       " ' مدیریت سنجاقی ',\n",
       " ' ولی نعمتان زمامداران ',\n",
       " ' قاسم سلیمانی ',\n",
       " ' رهنورد ',\n",
       " ' آزمون عدالت ',\n",
       " ' امتی ترین ',\n",
       " ' بقایی ',\n",
       " ' روحانی ',\n",
       " ' اسلامشهر ',\n",
       " ' گزارش بودجه ',\n",
       " ' شفافیت آرای نمایندگان ',\n",
       " ' اقدام و ',\n",
       " ' آیت الله رئیسی ',\n",
       " ' خان هفتم ',\n",
       " ' كره جنوبي ',\n",
       " ' جمنا ',\n",
       " ' خسارت محض ',\n",
       " ' نشر حداکثری ',\n",
       " ' خوزستان ',\n",
       " ' بانک ها ',\n",
       " ' خرید ملک ',\n",
       " ' امنیت ',\n",
       " ' میدون مردم ',\n",
       " ' نوروز۱۴۰۰ ',\n",
       " ' فساد. ',\n",
       " ' امام خامنه ای ',\n",
       " ' شیعه سنی ',\n",
       " ' پرستو ',\n",
       " ' سنگ قبر ',\n",
       " ' طلا ',\n",
       " ' رهبر ',\n",
       " ' مسلمان، ',\n",
       " ' مردمی سازی ',\n",
       " ' آمریکا! ',\n",
       " ' جنایت تروریستی اهواز ',\n",
       " ' موشک های بالستیک ',\n",
       " ' دولت پاسخگو ',\n",
       " ' البحرين ',\n",
       " ' مظلوم ',\n",
       " ' آتش سوزی زاهدان ',\n",
       " ' نظام سلطه ',\n",
       " ' بازنگری قانون اساسی ',\n",
       " ' مسكن ',\n",
       " ' رأی ',\n",
       " ' علیرضا زاکانی ',\n",
       " ' آشتی ملی ',\n",
       " ' برای سرباز ',\n",
       " ' شعار ',\n",
       " ' کربلا. ',\n",
       " ' جریان تحریف ',\n",
       " ' تحول ',\n",
       " ' بی انصافی ',\n",
       " ' جهش اقتصادی ',\n",
       " ' ابتکار ',\n",
       " ' توافق، ',\n",
       " ' انتخابات ۱۴۰۰ ',\n",
       " ' مجمع تشخیص مصلحت، ',\n",
       " ' سامانه  ',\n",
       " ' صدای مردم ',\n",
       " ' فلسطین! ',\n",
       " ' رشت ',\n",
       " ' کودک همسری ',\n",
       " ' محسنی اژه ای ',\n",
       " ' پاسخگويي ',\n",
       " ' اهواز ',\n",
       " ' رحمت ',\n",
       " ' الودگی هوا ',\n",
       " ' نقض برجام ',\n",
       " ' دادگاه علنی ',\n",
       " ' دختر ',\n",
       " ' آمريكا ',\n",
       " ' رییس جمهور ',\n",
       " ' حسن یزدانی ',\n",
       " ' ایران قوی، ',\n",
       " ' مقاومت ',\n",
       " ' بسیجی ',\n",
       " ' واردات ',\n",
       " ' جمهوری اسلامی ',\n",
       " ' ٢٨ مرداد ',\n",
       " ' ازدواج ',\n",
       " ' دروغ ممنوع ',\n",
       " ' محمودیه ',\n",
       " ' مرخصی ',\n",
       " ' مذاکره ',\n",
       " ' خمینی ',\n",
       " ' شیعه ',\n",
       " ' مانع زدایی ',\n",
       " ' نظام اسلامی، ',\n",
       " ' ساحل ',\n",
       " ' سینمایی ',\n",
       " ' رفاه ',\n",
       " ' تاثیر قطعی معدل ',\n",
       " ' طبری ',\n",
       " ' اوکراین ',\n",
       " ' درسهای تاریخ ',\n",
       " ' صربستان؛ ',\n",
       " ' جهانگیری ',\n",
       " ' جبران ',\n",
       " ' سازمان برنامه بودجه ',\n",
       " ' رجال ',\n",
       " ' مناظره۱۴۰۰ ',\n",
       " ' اعلیحضرتی ',\n",
       " ' تیم۱۱نفره ',\n",
       " ' جوانان ',\n",
       " ' ايران للشعب بصمة ',\n",
       " ' هاشمی رفسنجانی ',\n",
       " ' iran ',\n",
       " ' تجربه دولت دوازدهم ',\n",
       " ' کودتا  علیه جمهوریت ',\n",
       " ' آل سعود ',\n",
       " ' حکومت ',\n",
       " ' UNGA ',\n",
       " ' زبان انگلیسی ',\n",
       " ' إسرائيل ',\n",
       " ' آبان۹۸ ',\n",
       " ' توبه ',\n",
       " ' آزاد ',\n",
       " ' همتی! ',\n",
       " ' سپاه ',\n",
       " ' استان فارس ',\n",
       " ' مردم گله مندند ',\n",
       " ' یارانه ',\n",
       " ' کرمان ',\n",
       " ' Riyadh ',\n",
       " ' اطلاعیه ',\n",
       " ' سیدمحمدخاتمی، ',\n",
       " ' کافر عادل ',\n",
       " ' ویروس ',\n",
       " ' لاریجانی ',\n",
       " ' توزیع عادلانه ',\n",
       " ' روزنامه نگاران، ',\n",
       " ' انتصابات رهبری ',\n",
       " ' سازمان پزشکان بدون مرز ',\n",
       " ' اصفهان ',\n",
       " ' دانشجو ',\n",
       " ' پیامبر اکرم(ص) ',\n",
       " ' جفا ',\n",
       " ' اصلاح ',\n",
       " ' سرمایه داری ',\n",
       " ' الدول العربیة ',\n",
       " ' انتخابات۹۴ ',\n",
       " ' برنامه های بدون بودجه ',\n",
       " ' قاچاق ',\n",
       " ' طرح صیانت، ',\n",
       " ' تردید، ',\n",
       " ' روسيا ',\n",
       " ' ایران قوی ',\n",
       " ' شوراهای شهر ',\n",
       " ' امام خمینی ',\n",
       " ' اعتراف ',\n",
       " ' حق الناس ',\n",
       " ' سیستانی ',\n",
       " ' 16Day ',\n",
       " ' باکری) ',\n",
       " ' سیاست زدگی ',\n",
       " ' جهش ',\n",
       " ' islam ',\n",
       " ' مسؤلیت کیفری ',\n",
       " ' الریاض ',\n",
       " ' صالحي ',\n",
       " ' وزیر کشور: ',\n",
       " ' آل ',\n",
       " ' داووس ',\n",
       " ' معیشت ',\n",
       " ' هندبال زنان ',\n",
       " ' نشاط اجتماعی ',\n",
       " ' وام خرد ',\n",
       " ' غزة ',\n",
       " ' طالقانی ',\n",
       " ' نه جنگ نه مذاکره ',\n",
       " ' شرق ',\n",
       " ' گرانی شب عید ',\n",
       " ' تبلت ',\n",
       " ' سعودی ',\n",
       " ' خاورمیانه ',\n",
       " ' یمن ',\n",
       " ' Iranelection ',\n",
       " ' ویدئویی ',\n",
       " ' الصين ',\n",
       " ' دهه60، ',\n",
       " ' امریکا ',\n",
       " ' رشتو ',\n",
       " ' مطار ',\n",
       " ' صدرالساداتی ',\n",
       " ' حصر غیرقانونی ',\n",
       " ' تحلیف: ',\n",
       " ' اتوکشیده ',\n",
       " ' تحریم  ',\n",
       " ' قانون اساسی ',\n",
       " ' جمهوری اسلامی ایران ',\n",
       " ' ملی ترین ',\n",
       " ' بن سلمان ',\n",
       " ' جنگ نیابتی ',\n",
       " ' اهانت کنندگان ',\n",
       " ' NCAA ',\n",
       " ' دلواپسان ',\n",
       " ' رئیس ستادا   ف امام)می خواهم ',\n",
       " ' خبرخوب! ',\n",
       " ' کالا ',\n",
       " ' آرمان ',\n",
       " ' آزادی مخالف ',\n",
       " ' طرح ملی بیمه اجتماعی ',\n",
       " ' كلاب هاوس ',\n",
       " ' طرح تسهیل صدور مجوزهای کسب وکار ',\n",
       " ' دروغ ',\n",
       " ' رحمه للعالمین ',\n",
       " ' انقلابی ',\n",
       " ' فقها ',\n",
       " ' خوزستان تنها نیست ',\n",
       " ' تخریبی ',\n",
       " ' دادگاه ',\n",
       " ' آن چنان ',\n",
       " ' خراسان شمالی ',\n",
       " ' بداخلاقی ',\n",
       " ' کودتای انتخاباتی ',\n",
       " ' مجازی ',\n",
       " ' افغانستان ',\n",
       " ' اخلاق ',\n",
       " ' ایران\\u2066🇮🇷 ',\n",
       " ' احمد مسعود ',\n",
       " ' اقتصاد قوی ',\n",
       " ' گرسنگی ',\n",
       " ' الإمریکیة ',\n",
       " ' صدیقی!به ',\n",
       " ' چین ',\n",
       " ' جهانگیری، ',\n",
       " ' گارانتی ',\n",
       " ' رییس جمهور اقتصاددان ',\n",
       " ' شبه عمد) ',\n",
       " ' شهردار تهران ',\n",
       " ' دوگانه متعارض ',\n",
       " ' تَکرار ',\n",
       " ' سردار ',\n",
       " ' حلبچه ',\n",
       " ' راضي ',\n",
       " ' حذف ',\n",
       " ' مادر ',\n",
       " ' جوان ',\n",
       " ' تاریخ ',\n",
       " ' حاج احمد کاظمی ',\n",
       " ' استان هرمزگان ',\n",
       " ' fatf ',\n",
       " ' رئیس جمهور ',\n",
       " ' جرم سیاسی ',\n",
       " ' زنجان ',\n",
       " ' تولیدکنندگان ',\n",
       " ' زن ',\n",
       " ' مجلس ',\n",
       " ' به عمل کار برآید. ',\n",
       " ' اميد ',\n",
       " ' حق انتخاب ',\n",
       " ' دزدان دریایی ',\n",
       " ' شروط ضمن عقد ',\n",
       " ' تفاهم ',\n",
       " ' حکمرانی اقتصادی ',\n",
       " ' محسن فخری زاده ',\n",
       " ' دناپلاس ',\n",
       " ' سردار دلها ',\n",
       " ' جنسیت ',\n",
       " ' حفظ امنیت روانی جامعه ',\n",
       " ' فیروزجا ',\n",
       " ' مجلس، ',\n",
       " ' مدیران زن ',\n",
       " ' رئیسی، ',\n",
       " ' مسعودشجاعی ',\n",
       " ' دولت جوان حزب اللهی ',\n",
       " ' NeverThreatenAnIranian. ',\n",
       " ' عملکردها ',\n",
       " ' یار کمکی ',\n",
       " ' بزنیم زیر میز ',\n",
       " ' کالا برگ ',\n",
       " ' آبزیان ',\n",
       " ' ردصلاحیت ',\n",
       " ' توسعه سواد محیط زیستی ',\n",
       " ' خواست عمومی ',\n",
       " ' کاسبان رای ',\n",
       " ' نجفی ',\n",
       " ' حرف مردم ',\n",
       " ' امیرکبیر:حرکت ',\n",
       " ' پایان ',\n",
       " ' تغيیرات ملموس ',\n",
       " ' مبارک ',\n",
       " ' هاشمی ',\n",
       " ' دولت سوم روحانی ',\n",
       " ' مصلحت  ',\n",
       " ' letter4u ',\n",
       " ' روستا را حذف نکنید ',\n",
       " ' دکتر طحان نظیف ',\n",
       " ' شهید گنجی ',\n",
       " ' تولید ملی ',\n",
       " ' کارگران ',\n",
       " ' حلقه بسته ',\n",
       " ' انتخابات۱۴۰۰ ',\n",
       " ' آملی لاریجانی ',\n",
       " ' تاجزاده ',\n",
       " ' مکمل نگاه آمریکایی ',\n",
       " ' ايران ',\n",
       " ' سیلی ',\n",
       " ' امام صادق(ع) ',\n",
       " ' سورية ',\n",
       " ' انقلاب ',\n",
       " ' فصل هفتم ',\n",
       " ' ضد اقتصاد مقاومتی ',\n",
       " ' اعتراضات مسالمت آمیز ',\n",
       " ' پایان وس ',\n",
       " ' فردا برای شماست ',\n",
       " ' مناظره سوم ',\n",
       " ' پاسخگویی ',\n",
       " ' بانک ',\n",
       " ' خودروسازان جوان ',\n",
       " ' انتخابات، ',\n",
       " ' هم اکنون ',\n",
       " ' ناصر ملک مطیعی ',\n",
       " ' election94 ',\n",
       " ' سوم خرداد ',\n",
       " ' سازش ',\n",
       " ' بینوایان ',\n",
       " ' آزادی ',\n",
       " ' دروغ، ',\n",
       " ' بلای بزرگ ',\n",
       " ' اصلاح طلب، ',\n",
       " ' صافی گلپایگانی ',\n",
       " ' امنیت ملی ',\n",
       " ' Nowruz ',\n",
       " ' مهندسی انتخابات: ',\n",
       " ' هر نفر ده نفر ',\n",
       " ' ۱۶ آذر ',\n",
       " ' انحصار اقلیت ',\n",
       " ' نه جنگ نه تحریم\\u2069 ',\n",
       " ' صالحان ',\n",
       " ' منافقین ',\n",
       " ' ايرانيان ',\n",
       " ' دولت سایه\" ',\n",
       " ' تولید پشتیبانی ها مانع زدایی ها ',\n",
       " ' انقلاب اقتصادی ۱۴٠٠ ',\n",
       " ' زن،یک ',\n",
       " ' مخاطره ',\n",
       " ' کدخدا ',\n",
       " ' خاتمی حذف شدنی نیست ',\n",
       " ' نتانیاهو ',\n",
       " ' اموال تملیکی ',\n",
       " ' ارتجاع ',\n",
       " ' جنگ داخلی ',\n",
       " ' بمباران شیمیایی ',\n",
       " ' طلبکار ',\n",
       " ' سیدمحمدخاتمی ',\n",
       " ' ظلم ',\n",
       " ' مهاجری ',\n",
       " ' جملات کلیدی ',\n",
       " ' مسئولان کاخ نشین ',\n",
       " ' پرونده هسته ای ',\n",
       " ' محرم ',\n",
       " ' اسلام دین کامل ',\n",
       " ' اقتصاد مقاومتی ',\n",
       " ' هسته ',\n",
       " ' پیروزی ',\n",
       " ' هوشمندسازی ',\n",
       " ' همه پرسی ',\n",
       " ' womensday2021 ',\n",
       " ' نقدینگی ',\n",
       " ' عشق ',\n",
       " ' امام ',\n",
       " ' وعده ناممکن ندهیم ',\n",
       " ' روغن ',\n",
       " ' رياض ',\n",
       " ' استان کرمان ',\n",
       " ' اميركا ',\n",
       " ' دولت یازدهم ',\n",
       " ' ظلم آشکار ',\n",
       " ' فاتحان خیبر ',\n",
       " ' استانی شدن انتخابات ',\n",
       " ' ما دائر مدار منافعیم ',\n",
       " ' احمق قبلی ',\n",
       " ' سیاست خارجی ',\n",
       " ' وحدت ملی ',\n",
       " ' خودرو ',\n",
       " ' حکومت، ',\n",
       " ' کردستان عراق ',\n",
       " ' حقوق مردم ',\n",
       " ' تسلیت ',\n",
       " ' بهار ',\n",
       " ' حذف گزینه ',\n",
       " ' رفع تبعیض ',\n",
       " ' ریاست جمهوری، ',\n",
       " ' مکتب شهید سلیمانی ',\n",
       " ' کار نمایشی ',\n",
       " ' ظریف! ',\n",
       " ' زهرا نعمتی: ',\n",
       " ' کمیسیون کشاورزی، ',\n",
       " ' یادداشت ',\n",
       " ' آزادی خواه ',\n",
       " ' مراجع تقلید ',\n",
       " ' گفتگوی ملی ',\n",
       " ' زاگرس در آتش ',\n",
       " ' رهبری ',\n",
       " ' اولویت سرپرست ',\n",
       " ' واکسن برکت ',\n",
       " ' لإيران ',\n",
       " ' حضرت زهرا(س) ',\n",
       " ' حکم رهبرمعظم ',\n",
       " ' استان خوزستان ',\n",
       " ' نه به خشونت علیه زنان ',\n",
       " ' نه به حرف درمانی ',\n",
       " ' رد صلاحیت ',\n",
       " ' تغییر ریل بودجه ',\n",
       " ' زنان ',\n",
       " ' علمدار ',\n",
       " ' حسین خرازی ',\n",
       " ' خدمت سربازی ',\n",
       " ' اليمن. ',\n",
       " ' دموکراسی ',\n",
       " ' استان مرکزی ',\n",
       " ' بحران آب ',\n",
       " ' احمدی نژاد ',\n",
       " ' برنامه ',\n",
       " ' نظرسنجی: ',\n",
       " ' شهید، ',\n",
       " ' بدون استثنا ',\n",
       " ' نسل چهارم ',\n",
       " ' احمدی نژاد ',\n",
       " ' MiddleEast. ',\n",
       " ' تولیدکنندگان، ',\n",
       " ' استان کردستان ',\n",
       " ' صدیقی ',\n",
       " ' حق مردم در جیب مردم ',\n",
       " ' گزارش به مردم ',\n",
       " ' دکتر جبلی ',\n",
       " ' مقاومت ایرانیان ',\n",
       " ' نیویورک تایمز:مقالات ',\n",
       " ' راستی آزمایی واقعی ',\n",
       " ' غربالگری ژنتیک ',\n",
       " ' آموزش و پرورش ',\n",
       " ' گرانی ',\n",
       " ' جمهوری، ',\n",
       " ' قطع برق ',\n",
       " ' استان لرستان ',\n",
       " ' جورج فلوید ',\n",
       " ' تروريسم ',\n",
       " ' بودجه۱۴۰۰ ',\n",
       " ' مستضعفین ',\n",
       " ' سپاه، ',\n",
       " ' مردم بی پناه ',\n",
       " ' حداقل دستمزد ',\n",
       " ' زدن زیر میز ',\n",
       " ' عراق: ',\n",
       " ' کردستان ',\n",
       " ' Iran ',\n",
       " ' بوکوحرام ',\n",
       " ' مجلس قوی ',\n",
       " ' محافظه کاری ',\n",
       " ' کاهش تبعیض ',\n",
       " ' ایران؟ ',\n",
       " ' صفقة القرن» ',\n",
       " ' شگفتی ',\n",
       " ' فاجعه ملی ',\n",
       " ' خانواده ',\n",
       " ' ما میتوانیم ',\n",
       " ' جهادی ',\n",
       " ' اقتصاد ضعیف ',\n",
       " ' جلیلی ',\n",
       " ' یک میلیون واحد مسکن ',\n",
       " ' موسوی ',\n",
       " ' اصلاحات. ',\n",
       " ' استان البرز ',\n",
       " ' معلولان ',\n",
       " ' دولت مردمی ',\n",
       " ' عقلانی ',\n",
       " ' سپاه قدس ',\n",
       " ' ارز ',\n",
       " ' دولت سلام ',\n",
       " ' ایران مان ',\n",
       " ' شاهکار روحانی ',\n",
       " ' دولت اقدام و تحول ',\n",
       " ' رانت ',\n",
       " ' سرکوب درمانی ',\n",
       " ' معوقات بانکی ',\n",
       " ' کرونا را شکست میدهیم. ',\n",
       " ' ملکه الیزابت ',\n",
       " ' نظرمردم ',\n",
       " ' سکوت ',\n",
       " ' ترور ',\n",
       " ' بارزانی ',\n",
       " ' فرار مالیاتی ',\n",
       " ' حریم خصوصی ',\n",
       " ' بورس ',\n",
       " ' شجریان، ',\n",
       " ' علی لندی ',\n",
       " ' ایران  اینترنشنال: ',\n",
       " ' عدالت ',\n",
       " ' حقوق خانه داری ',\n",
       " ' IranElection2021 ',\n",
       " ' سعید جلیلی ',\n",
       " ' امام خمینی(ره)، ',\n",
       " ' ممانعت ',\n",
       " ' کار برای مردم ',\n",
       " ' نبویان ',\n",
       " ' مرگ برجام ',\n",
       " ' دولت جوان و حزب اللهی ',\n",
       " ' باهنر ',\n",
       " ' بوشهر ',\n",
       " ' صدا و سیما ',\n",
       " ' سیاسی ',\n",
       " ' رومینا اشرفی ',\n",
       " ' حاج قاسم ',\n",
       " ' دانشجویان ',\n",
       " ' دولت ',\n",
       " ' هسته ای ',\n",
       " ' حزب الله ',\n",
       " ' زيارة اربعين ',\n",
       " ' تراشه ',\n",
       " ' غ ',\n",
       " ' صهیونیستی ',\n",
       " ' جوانگرایی ',\n",
       " ' حجاب ',\n",
       " ' بنیاد شهید ',\n",
       " ' غنی سازی ',\n",
       " ' شهر کشور ',\n",
       " ' ترامب ',\n",
       " ' بصیرت، ',\n",
       " ' ۱۴۰۰ ',\n",
       " ' بحران سازی ',\n",
       " ' انتصابات ',\n",
       " ' تیم ملت ایران ',\n",
       " ' 3YearsOfWarOnYemen ',\n",
       " ' سینمای بعد از انقلاب ',\n",
       " ' سیاست های اقتصادی دانش ',\n",
       " ' اتاق بازرگانی، ',\n",
       " ' اصلاحات ',\n",
       " ' ضعف ایمان ',\n",
       " ' صادق لاریجانی ',\n",
       " ' دستگاه قضایی ',\n",
       " ' انتخاب درست کار درست ',\n",
       " ' اللهم عجل لوليك الفرج ',\n",
       " ' مصادره ',\n",
       " ' تنها گریه کن ',\n",
       " ' سراج ',\n",
       " ' بنی صدر ',\n",
       " ' ظلم و جفا ',\n",
       " ' نقد ',\n",
       " ' پدیده   ',\n",
       " ' پنجشير ',\n",
       " ' حداكثري ',\n",
       " ' غزل ',\n",
       " ' گاندو  ',\n",
       " ' شرمن ',\n",
       " ' جنایت غیرعمد ',\n",
       " ' فیلتر ',\n",
       " ' حصر غیرقانونی، ',\n",
       " ' دل تنگی ',\n",
       " ' قطعی برق ',\n",
       " ' مشارکت حداکثری ',\n",
       " ' امتیازاتی ',\n",
       " ' مافیای خودرو ',\n",
       " ' طبیعت ',\n",
       " ' کاندیدای پوششی ',\n",
       " ' گام دوم ',\n",
       " ' Arbaeen2020 ',\n",
       " ' دوقطبی ',\n",
       " ' ریاست جمهوری ',\n",
       " ' جرم ',\n",
       " ' میدان ',\n",
       " ' مشايي ',\n",
       " ' حبس ',\n",
       " ' غربگرایی ',\n",
       " ' جهش تولید ',\n",
       " ' امام جمعه اصفهان، ',\n",
       " ' لایحه ی شفافیت ',\n",
       " ' ليلة القدر ',\n",
       " ' پلیس ',\n",
       " ' شفافیت. ',\n",
       " ' کتاب ',\n",
       " ' 24خرداد ',\n",
       " ' پیشرفت همه جانبه ',\n",
       " ' داعش، ',\n",
       " ' حميد بقایی ',\n",
       " ' امنیت ملی ایران ',\n",
       " ' اورژانس اجتماعی ',\n",
       " ' دانشگاه ',\n",
       " ' نظارت استصوابی ',\n",
       " ' بازگشت به مردم ',\n",
       " ' اسلامی ',\n",
       " ' خودی غیرخودی، ',\n",
       " ' گوهر عشقی ',\n",
       " ' راستی آزمایی واقعی ',\n",
       " ' استعفا ',\n",
       " ' دولت توانا در جامعه توانا ',\n",
       " ' شبکه یک ',\n",
       " ' تاریخ کرونا ',\n",
       " ' مردم ',\n",
       " ' مرزبان ',\n",
       " ' جامعه انسانی ',\n",
       " ' مسلماً ',\n",
       " ' نفت ',\n",
       " ' شهدا، ',\n",
       " ' دولتی ',\n",
       " ' امارات\\u2069 ',\n",
       " ' بانیان وضع موجود ',\n",
       " ' آیت الله هاشمی رفسنجانی ',\n",
       " ' استان یزد ',\n",
       " ' بنزین ',\n",
       " ' پیمان جبلی ',\n",
       " ' پ ',\n",
       " ' ایرلو ',\n",
       " ' گام دوم انقلاب ',\n",
       " ' چهل سالگی انقلاب ',\n",
       " ' سعدی شیرازی ',\n",
       " ' فریدون ',\n",
       " ' کیهان، ',\n",
       " ' تخم مرخ ',\n",
       " ' توجیه ',\n",
       " ' EconomicTerrorism ',\n",
       " ' گوشی هوشمند ',\n",
       " ' خانه های خالی ',\n",
       " ' رهبر انقلاب ',\n",
       " ' اتحاد ملی ',\n",
       " ' دوگانه طبیعی ',\n",
       " ' فروش اوراق نفتی ',\n",
       " ' هرنفر۱۰نفر ',\n",
       " ' دیپلماسی ',\n",
       " ' خیزش مستضعفان ',\n",
       " ' گیلان ',\n",
       " ' چندسال ',\n",
       " ' مسکن ',\n",
       " ' منطقه ',\n",
       " ' مرکل ',\n",
       " ' قرون وسطی ',\n",
       " ' جهش بزرگ ',\n",
       " ' مناظره  ها ',\n",
       " ' پادگان ',\n",
       " ' کودتا ',\n",
       " ' باید ',\n",
       " ' اربعین بدون دلار ',\n",
       " ' انزوا ',\n",
       " ' صداوسيما ',\n",
       " ' انتقاد ',\n",
       " ' مرجع بصیر ',\n",
       " ' پیشرفت کشور ',\n",
       " ' پنج دربرابر یک ',\n",
       " ' اعتراض ',\n",
       " ' فلسطين ',\n",
       " ' خراسان رضوی ',\n",
       " ' فرودگاه ',\n",
       " ' خیر موثر ',\n",
       " ' سیاست گذاری ',\n",
       " ' معطل ',\n",
       " ' موساد ',\n",
       " ' چهارمحال و بختیاری ',\n",
       " ' ديپلماسي ',\n",
       " ' انقلاب اسلامی ',\n",
       " ' قاسم سلیمانی قهرمان ملی ',\n",
       " ' غلط ',\n",
       " ' القدس اقرب ',\n",
       " ' رسانه باشید ',\n",
       " ' رسانه های جریان اصلی ',\n",
       " ' دولت دهم ',\n",
       " ' EconomicTerrorism. ',\n",
       " ' دختر آبی ',\n",
       " ' جاسوسی ',\n",
       " ' أمريكا ',\n",
       " ' اعلامیه جهانی حقوق بشر ',\n",
       " ' فضای مجازی ',\n",
       " ' دولت سایه ',\n",
       " ' سرپل ذهاب ',\n",
       " ' غرب ',\n",
       " ' ProphetMuhammad ',\n",
       " ' من رای میدهم ',\n",
       " ' مقاومت پنجشیر ',\n",
       " ' بحرين ',\n",
       " ' كلا للإعدام ',\n",
       " ' منفعل، ',\n",
       " ' حاج قاسم سلیمانی ',\n",
       " ' دشمنان ',\n",
       " ' طالبان ',\n",
       " ' استعمارگر ',\n",
       " ' والیبال ',\n",
       " ' جلیلی-دانشگاه ',\n",
       " ' هم اکنون ',\n",
       " ' موافقت ',\n",
       " ' حمایت ',\n",
       " ' صلح، ',\n",
       " ' داعش ',\n",
       " ' روز جهانی کارگر ',\n",
       " ' اصلاح طلب: ',\n",
       " ' زلزله ',\n",
       " '  کرونا ',\n",
       " ' من رویا دارم پس هستم. ',\n",
       " ' انتقام سخت ',\n",
       " ' رژیم صهیونیستی: ',\n",
       " ' ترامپ ',\n",
       " ' سوابق تحصیلی ',\n",
       " ' برادرکشی ',\n",
       " ' خاتمی مدیا ',\n",
       " ' سازمان نظامی رسمی ',\n",
       " ' حمایت از کالای ایرانی ',\n",
       " ' دولت آینده ',\n",
       " ' روز ملی شوراها ',\n",
       " ' بایدن ',\n",
       " ' سازمان شانگهای ',\n",
       " ' اهل سنت ',\n",
       " ' بعثت ',\n",
       " ' هر گوشی یک ستاد ',\n",
       " ' آذربایجان شرقی ',\n",
       " ' حق انتقاد ',\n",
       " ' شینزو آبه ',\n",
       " ' دولت سوم خاتمی ',\n",
       " ' پیک چهارم ',\n",
       " ' جناب ',\n",
       " ' رکود ',\n",
       " ' هفت تپه ',\n",
       " ' رئیس جمهور، ',\n",
       " ' صاحب الزمان ',\n",
       " ' قرآن كريم ',\n",
       " ' غیر پوپولیستی ',\n",
       " ' محسن رضایی:با ',\n",
       " ' رای اولی ',\n",
       " ' طرح اعاده اموال نامشروع مسئولان ',\n",
       " ' پیامک وحشت ',\n",
       " ' سمنان ',\n",
       " ' توئيتر ',\n",
       " ' مرغ ',\n",
       " ' انتخابات حداقلی ',\n",
       " ' هیات مقررات زدایی ',\n",
       " ' اکثریت غیرخودی ',\n",
       " ' اشتغال ',\n",
       " ' سیف ',\n",
       " ' سازمان برنامه وبودجه ',\n",
       " ' دفن زباله ',\n",
       " ' پاسخ معتبر ',\n",
       " ' تجربیات دانشگاه ',\n",
       " ' تهران: ',\n",
       " ' گشایش اقتصادی ',\n",
       " ' تجارت آموزشی ',\n",
       " ' احمد گل محمدی ',\n",
       " ' ترکيا ',\n",
       " ' طرح صیانت ',\n",
       " ' نیروی دریایی ',\n",
       " ' آل سعود ',\n",
       " ' افغانستانی ',\n",
       " ' جنگ اقتصادي ',\n",
       " ' تولید ',\n",
       " ' تحریم، ',\n",
       " ' Future Bank ',\n",
       " ' دولت سیزدهم ',\n",
       " ' اسرائیل ',\n",
       " ' مدیریت داشته ها ',\n",
       " ' عدالت فرهنگی ',\n",
       " ' عذرخواهی ',\n",
       " ' اصغرزاده، ',\n",
       " ' شورای نگهبان: ',\n",
       " ' لباس شخصی ها ',\n",
       " ' iranelections ',\n",
       " ' محاصره ',\n",
       " ' هدم جمهوریت ',\n",
       " ' مریم میرزاخانی ',\n",
       " ' عدالت جنسیتی ',\n",
       " ' بانکی ',\n",
       " ' مبارزه بافساد ',\n",
       " ' صداقت بجای دروغ ',\n",
       " ' انگلیس ',\n",
       " ' رقابت ۹۶ ',\n",
       " ' دارایی های ارزی ',\n",
       " ' ارزپاشی ',\n",
       " ' گلوبال هاوک، ',\n",
       " ' دهه نودی ها ',\n",
       " ' تصمیمات ',\n",
       " ' فرصت های ازدست رفته ',\n",
       " ' اصلاح طلب ',\n",
       " ' نماز جمعه ',\n",
       " ' ویدئو ',\n",
       " ' امنیت غذایی ',\n",
       " ' استان سمنان ',\n",
       " ' امارات ',\n",
       " ' شورای نگهبان ',\n",
       " ' عادل کیانپور ',\n",
       " ' دیکتاتوری دلار ',\n",
       " ' آمریکا ',\n",
       " ' پيشنهاد ',\n",
       " ' تینا پاکروان ',\n",
       " ' ورزشکاران ',\n",
       " ' توهین به رئیس  جمهور ',\n",
       " ' زاکانی ',\n",
       " ' رقابت ناسالم ',\n",
       " ' شوراي نگهبان ',\n",
       " ' براندازی ',\n",
       " ' تحریم ها ',\n",
       " ' هندبال ',\n",
       " ' هفت نه ',\n",
       " ' شورای امنیت ',\n",
       " ' نفوذ ',\n",
       " ' واکسن کرونا ',\n",
       " ' حکیم ',\n",
       " ' کشور ',\n",
       " ' فرجی دانا ',\n",
       " ' مادران چشم انتظار ',\n",
       " ' شیطان ',\n",
       " ' اوکراین: ',\n",
       " ' IranTalksVienna ',\n",
       " ' حقوق مؤلف ',\n",
       " ' خبرخوب، ',\n",
       " ' ستاد مهندسی تظاهرات ',\n",
       " ' قطعنامه ',\n",
       " ' آل خليفه ',\n",
       " ' اصول گرا ',\n",
       " ' حقوق ',\n",
       " ' حصر ',\n",
       " ' نرمش قهرمانانه ',\n",
       " ' این یعنی یک اشکالی هست ',\n",
       " ' زهرا شجاعی ',\n",
       " ' قطعنامه های متعدد ',\n",
       " ' مسلمانان مظلوم هند ',\n",
       " ' وحدت ',\n",
       " ' مواضع خطرناک ',\n",
       " ' يهوديان ',\n",
       " ' انقلاب رنگی ',\n",
       " ' چپ گرایی اقتصادی ',\n",
       " ' اربعین ',\n",
       " ' استاد پناهیان ',\n",
       " ' گفتگو ',\n",
       " ' حافظ ',\n",
       " ' اصولگرایی ',\n",
       " ' ارتش ',\n",
       " ' دلار جهانگیری ',\n",
       " ' إيران ',\n",
       " ' سيلي اول؛ ',\n",
       " ' تورم  ',\n",
       " ' عنابستانی: ',\n",
       " ' تهدید ',\n",
       " ' البروتوكول الإضافي ',\n",
       " ' ۱۳ آبان ',\n",
       " ' نظرسنجی ',\n",
       " ' FreeIranianSoldiers ',\n",
       " ' حاکمیت قانون، ',\n",
       " ' رفراندم ',\n",
       " ' وزارت صمت ',\n",
       " ' election ',\n",
       " ' فتنه ',\n",
       " ' IranTalks ',\n",
       " ' النصر ',\n",
       " ' ایران جدید ',\n",
       " ' خانه داری شغل است ',\n",
       " ' روحانی، ',\n",
       " ' تنظیم بازار ',\n",
       " ' بودجه ',\n",
       " ' مجلس یازدهم ',\n",
       " ' کاندیداهای ',\n",
       " ' فرهنگی ',\n",
       " ' اعمالی ',\n",
       " ' دومین نمایشگاه مجازی کتاب ',\n",
       " ' تاجیکستان ',\n",
       " ' تالش ',\n",
       " ' خرمشهر ',\n",
       " ' رسول ',\n",
       " ' مسلمان ظالم ',\n",
       " ' روایت تحریم ',\n",
       " ' PS752 ',\n",
       " ' ملت ',\n",
       " ' روسیه ',\n",
       " ' کار ',\n",
       " ' خیانت ',\n",
       " ' زندان ',\n",
       " ' مذاكره ',\n",
       " ' زور ما می رسد ',\n",
       " ' سرمربی توانمند ',\n",
       " ' ترک تابعیت ',\n",
       " ' شریک ',\n",
       " ' خامنه ای ',\n",
       " ' هرمزگان ',\n",
       " ' العراقية ',\n",
       " ' تهمت ',\n",
       " ' وین ',\n",
       " ' جمهوری ',\n",
       " ' ۱۳آبان ',\n",
       " ' IranTeam’s ',\n",
       " ' امام موسی صدر ',\n",
       " ' حذف غیرقانونی ',\n",
       " ' لیبرال، ',\n",
       " ' الإتفاق النووي. ',\n",
       " ' لیست جمهور ',\n",
       " ' کمالوندی ',\n",
       " ' محاکمه ',\n",
       " ' بیت المال ',\n",
       " ' الإمارات ',\n",
       " ' انرژی ',\n",
       " ' اقتصادی ',\n",
       " ' لغو ',\n",
       " ' زیر میز ایران هراسی ',\n",
       " ' بشار اسد ',\n",
       " ' حج ',\n",
       " ' امتداد روحانی ',\n",
       " ' کارنامه روحانی در حوزه زنان ',\n",
       " ' جهرمی ',\n",
       " ' رای اشتباه ',\n",
       " ' نمی توانی ',\n",
       " ' حسابهای سپرده قوه قضاییه ',\n",
       " ' مسقط، ',\n",
       " ' مطبوعات ',\n",
       " ' اقتصاد تحریم ',\n",
       " ' محسن رضایی ',\n",
       " ' هواپیمای اوکراینی ',\n",
       " ' صدای بی قدرتان ',\n",
       " ' پدافند غیرعامل ',\n",
       " ' حذف دلار ',\n",
       " ' مفسدان اقتصادی ',\n",
       " ' ما ملت امام حسینیم ',\n",
       " ' واکسن آلوده آمریکایی ',\n",
       " ' رائے شماری ',\n",
       " ' خشونت علیه زنان ',\n",
       " ' آتش به اختیار ',\n",
       " ' سیلی سپاه ',\n",
       " ' ژاپن ',\n",
       " ' همرزمان ',\n",
       " ' سنصلي فی القدس ',\n",
       " ' خامنه ای: ',\n",
       " ' مجازات ',\n",
       " ' قدرتمند ',\n",
       " ' TokyoParalympics2020 ',\n",
       " ' دولت نظامیان، ',\n",
       " ' لبنان ',\n",
       " ' طرح وان ',\n",
       " ' فساد خودی ',\n",
       " ' کیف کش! ',\n",
       " ' اوراق بدهی ',\n",
       " ' جراحی درون نظام ',\n",
       " ' آلمان ',\n",
       " ' تنگه هرمز ',\n",
       " ' استقراض غیرمستقیم ',\n",
       " ' ریاست جمهوری، ',\n",
       " ' انقلاب اقتصادی ۱۴۰۰ ',\n",
       " ' اشد مجازات ',\n",
       " ' قهرمان ',\n",
       " ' سیستان وبلوچستان ',\n",
       " ' زندانیان ',\n",
       " ' سفیر ',\n",
       " ' بی اثرسازی تحریم ها ',\n",
       " ' دوشغله( ',\n",
       " ' مالیاتی ',\n",
       " ' کلاب هاوس ',\n",
       " ' شفاف سازی ',\n",
       " ' آقا خوشش بیاید، ',\n",
       " ' رابطه ',\n",
       " ' تحریم داخلی ',\n",
       " ' نمی تواند ',\n",
       " ' عزت ',\n",
       " ' نامه رئیس قوه قضائیه ',\n",
       " ' تحريم ',\n",
       " ' سعدی: ',\n",
       " ' کامیونداران ',\n",
       " ' ضد تبعیض ',\n",
       " ' مرا دریاب ',\n",
       " ' ترامب! ',\n",
       " ' فساد ',\n",
       " ' وزیر اطلاعات ',\n",
       " ' حامد امیری ',\n",
       " ' آذربایجان غربی ',\n",
       " ' حمید هوشنگی ',\n",
       " ' دموکراسي ',\n",
       " ' آرامش ',\n",
       " ' سهام عدالت. ',\n",
       " ' HERO ',\n",
       " ' رانت و انحصار ',\n",
       " ' نامزدها ',\n",
       " ' اینستاگرام ',\n",
       " ' الزامات جمهوری ',\n",
       " ' رفع تحریم ',\n",
       " ' رژیم صهیونیستی ',\n",
       " ' اصلاحات ساختاری ',\n",
       " ' فلسطین حل= ',\n",
       " ' شریفی ',\n",
       " ' بولتون ',\n",
       " ' حاشیه سازی ',\n",
       " ' جنگ نرم ',\n",
       " ' عباس عبدی ',\n",
       " ' رهایی بخش ',\n",
       " ' رای میدهم ',\n",
       " ' روحانیت ',\n",
       " ' سیستان و بلوچستان ',\n",
       " ' شورای سنجش ',\n",
       " ' کار آفرینی ',\n",
       " ' قوه قضائیه ',\n",
       " ' نه به پلاستیک ',\n",
       " ' اربعینی ها ',\n",
       " ' تصمیمات محفلی ',\n",
       " ' هجده تیر ',\n",
       " ' رفع تحريم ',\n",
       " ' استان گیلان ',\n",
       " ' احمدی نژاد، ',\n",
       " ' شخصیت سیاسی مذهبی ',\n",
       " ' رئیس جمهور ',\n",
       " ' پروژه نفوذ ',\n",
       " ...]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert multihahstags to words\n",
    "hashtag = []\n",
    "for ht_list in data['hashtags']:\n",
    "    for ht in ht_list:\n",
    "        hashtag.append(ht)\n",
    "hash_set = set(hashtag)\n",
    "ht_list = []\n",
    "for h in hash_set:\n",
    "    ht_list.append(h)\n",
    "hash_list = []\n",
    "for h in hash_set:\n",
    "    hash_list.append(' '+ ' '.join(h[1:].split('_')) + ' ')\n",
    "hash_list2 = []\n",
    "for h in hash_list:\n",
    "    hash_list2.append(' '.join(h.split('\\u200c')))\n",
    "hash_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d2a985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    if data['tweet_num_hashtags'].iloc[i] > 0:\n",
    "        for h in data['hashtags'].iloc[i]:\n",
    "            ind = ht_list.index(h)\n",
    "            data['full_text'].iloc[i] = data['full_text'].iloc[i].replace(h, hash_list2[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c6140a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    tokenizer = TweetTokenizer()\n",
    "    stop_words = list()\n",
    "    with open(r'C:/PRIVATE/Metodata/Metodata-Files/stopwords_new.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            stop_words.append(line.strip())\n",
    "    exclusions = [\"ir\",\"#ff\", \"ff\", \"rt\",\"RT\", \"FF\",\"\\u200c\",\"\\n\",\"'s\",\"n't\",\"'re\",\"'m\",'#','@','&','?','.','+','-','*','/','’','...','…','‘','“','”','–','؟','،','.','\"',';','!',':','%','.',',']\n",
    "    stop_words.extend(exclusions)\n",
    "\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    url_regex = r\"\"\"((?:https?://)?(?:(?:www\\.)?(?:[\\da-z\\.-]+)\\.(?:[a-z]{2,6})|(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)|(?:(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|:(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(?:ffff(?::0{1,4}){0,1}:){0,1}(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|(?:[0-9a-fA-F]{1,4}:){1,4}:(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])))(?::[0-9]{1,4}|[1-5][0-9]{4}|6[0-4][0-9]{3}|65[0-4][0-9]{2}|655[0-2][0-9]|6553[0-5])?(?:/[\\w\\.-]*)*/?)\\b\"\"\"\n",
    "    RFC_5322_COMPLIANT_EMAIL_REGEX = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    \n",
    "    remove_url = (url_regex, ' ')\n",
    "    remove_email = (RFC_5322_COMPLIANT_EMAIL_REGEX, ' ')\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_before = compile_patterns([remove_url, remove_email])\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    \n",
    "    text = text.lower()\n",
    "    for pattern, repl in compiled_patterns_before:\n",
    "        text = pattern.sub(repl, text)\n",
    "    text = re.sub(r'[\\u200c\\s]*\\s[\\s\\u200c]*', ' ', text)\n",
    "    text = re.sub(r'[\\u200c]+', '\\u200c', text)\n",
    "\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    tokenized_words = tokenizer.tokenize(text)\n",
    "    tokenized_words = [word for word in tokenized_words if word not in stop_words]\n",
    "\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_emoji(text):\n",
    "    junk_chars_regex = r'[^a-zA-Z\\u0621-\\u06CC\\u0698\\u067E\\u0686\\u06AF \\u200c]'\n",
    "    compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "    remove_junk_characters = (junk_chars_regex, ' ')\n",
    "    compiled_patterns_after = compile_patterns([remove_junk_characters])\n",
    "    for pattern, repl in compiled_patterns_after:\n",
    "        text = pattern.sub(repl, text)\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def make_farsi_text(x):\n",
    "    reshaped_text = arabic_reshaper.reshape(x)\n",
    "    farsi_text = get_display(reshaped_text)\n",
    "    return farsi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "65516bd5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 50)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:6: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5080/2797418508.py:7: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "data['full_text']=data['full_text'].fillna('')\n",
    "print(data.shape)\n",
    "data['full_text'] = [str(i).lower() for i in data['full_text']]\n",
    "print('1')\n",
    "data['full_text']=data['full_text'].str.replace('\\d+', '')\n",
    "data['full_text'] = data['full_text'].str.replace('@[\\w\\-]+','')\n",
    "print('2')\n",
    "data['full_text']=data['full_text'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "print('3')\n",
    "data['full_text']=data['full_text'].apply(lambda x:remove_emoji(x) )\n",
    "print('4')\n",
    "data['full_text']=data['full_text'].apply(lambda x:preprocess(x))\n",
    "print('5')\n",
    "data['full_text']=data['full_text'].apply(lambda x:' '.join([word for word in x]))\n",
    "print('6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce9516",
   "metadata": {},
   "source": [
    "# Negativity/Character Attack/Political Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bd5192ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Character Attack and Political Attack run following comment\n",
    "\n",
    "data = data[data['neg'] == 1]\n",
    "\n",
    "#for Character Attack change data['neg'] to data['char'] in code\n",
    "#for Political Attack change data['neg'] to data['pol'] in code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1197da",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e55a2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['neg'], test_size=0.2, random_state=0, stratify=data['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2a8ce",
   "metadata": {},
   "source": [
    "## Undersmapling - Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "4d626faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 50)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_data = X_train[X_train['neg'] == 1]\n",
    "positive_data = X_train[X_train['neg'] == 0]\n",
    "'''\n",
    "#Undersampling\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)'''\n",
    "\n",
    "#Oversampling\n",
    "'''if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "\n",
    "X_train = pd.concat([negative_data, positive_data])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccf2fa",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e6212911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 300)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_train = []\n",
    "for w in X_train['full_text']:\n",
    "    vecs_n_train.append(ft.get_word_vector(w))\n",
    "vecs_n_train = np.array(vecs_n_train)\n",
    "vecs_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "320a28af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 300)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_n_test = []\n",
    "for w in X_test['full_text'].to_list():\n",
    "    vecs_n_test.append(ft.get_word_vector(w))\n",
    "vecs_n_test = np.array(vecs_n_test)\n",
    "vecs_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "4fa0d51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831,)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_train = X_train['neg'].to_list()\n",
    "y_n_train = np.array(y_n_train)\n",
    "y_n_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d547a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n_test = y_test\n",
    "y_n_test = np.array(y_n_test)\n",
    "y_n_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "088b4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index()\n",
    "X_test = X_test.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf0af4",
   "metadata": {},
   "source": [
    "## Textual Features for Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "7cd4c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.DataFrame({'full_text':X_train['full_text'], 'neg':X_train['neg']})\n",
    "df_s1 = S[S['neg'] == 1]\n",
    "df_s0 = S[S['neg'] == 0]\n",
    "def Merge(D1,D2):\n",
    "    py = D1 | D2\n",
    "    return py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "accfab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sort_Dic = {}\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l1 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l2 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(X_train['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "l3 = list(sort_dic.items())\n",
    "sort_Dic = Merge(sort_Dic,sort_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "616ba2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 850)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_train['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_train = pd.DataFrame(vecs_n_train)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_train = vecs_n_train.reset_index()\n",
    "dic_all_train = pd.concat([vecs_n_train, df_top_s], axis=1)\n",
    "dic_all_train = dic_all_train.drop({'index'}, axis=1)\n",
    "dic_all_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "634faa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s0['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic0_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (1,1)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_1 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (2,2)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_2 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "cv = CountVectorizer(ngram_range= (3,3)) \n",
    "cv_fit = cv.fit_transform(df_s1['full_text'].to_list())\n",
    "len(cv.get_feature_names())\n",
    "A = cv_fit.toarray()\n",
    "value = list(A.sum(axis=0))\n",
    "key = list(cv.get_feature_names())\n",
    "z = zip(key , value)\n",
    "dic = dict(z)\n",
    "sort_dic1_3 = dict(sorted(dic.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "sort_dic0_n = Merge(sort_dic0_1, sort_dic0_2)\n",
    "sort_dic0 = Merge(sort_dic0_n, sort_dic0_3)\n",
    "\n",
    "sort_dic1_n = Merge(sort_dic1_1, sort_dic1_2)\n",
    "sort_dic1 = Merge(sort_dic1_n, sort_dic1_3)\n",
    "\n",
    "sort_dic0 = dict(sorted(sort_dic0.items(), key=lambda item: item[1] , reverse=True))\n",
    "sort_dic1 = dict(sorted(sort_dic1.items(), key=lambda item: item[1] , reverse=True))\n",
    "\n",
    "Top0 = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1 = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]\n",
    "\n",
    "Top01 = [x for x in list(sort_dic0_1.keys()) if x not in sort_dic1_1.keys()][:500] + [x for x in list(sort_dic0_2.keys()) if x not in sort_dic1_2.keys()][:100] + [x for x in list(sort_dic0_3.keys()) if x not in sort_dic1_3.keys()][:100]\n",
    "Top10 = [x for x in list(sort_dic1_1.keys()) if x not in sort_dic0_1.keys()][:500] + [x for x in list(sort_dic1_2.keys()) if x not in sort_dic0_2.keys()][:100] + [x for x in list(sort_dic1_3.keys()) if x not in sort_dic0_3.keys()][:100]\n",
    "\n",
    "Top0_w = [x for x in list(sort_dic0_1.keys())][:500] + [x for x in list(sort_dic0_2.keys())][:100] + [x for x in list(sort_dic0_3.keys())][:50]\n",
    "Top1_w = [x for x in list(sort_dic1_1.keys())][:500] + [x for x in list(sort_dic1_2.keys())][:100] + [x for x in list(sort_dic1_3.keys())][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "98636e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(831, 831, 831, 831, 831, 831)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0.append(count)\n",
    "    else:\n",
    "        feature_Top0.append(0)\n",
    "        \n",
    "feature_Top1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1.append(count)\n",
    "    else:\n",
    "        feature_Top1.append(0)\n",
    "        \n",
    "feature_Top01 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01.append(count)\n",
    "    else:\n",
    "        feature_Top01.append(0)\n",
    "        \n",
    "feature_Top10 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10.append(count)\n",
    "    else:\n",
    "        feature_Top10.append(0)\n",
    "        \n",
    "feature_weight1 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1.append(count)\n",
    "    else:\n",
    "        feature_weight1.append(0)\n",
    "\n",
    "feature_weight0 = []\n",
    "for i in range(len(X_train['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_train['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_train['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0.append(count)\n",
    "    else:\n",
    "        feature_weight0.append(0)\n",
    "\n",
    "len(feature_Top0), len(feature_Top1), len(feature_Top01), len(feature_Top10), len(feature_weight1), len(feature_weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "f17353ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((831, 868), 831)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0,\n",
    "     '50Top_1': feature_Top1,\n",
    "     'Top_0_1': feature_Top01,\n",
    "     'Top_1_0': feature_Top10,\n",
    "     'Weight_0': feature_weight0,\n",
    "     'Weight_1': feature_weight1})\n",
    "features_train['tweet_like'] = X_train['tweet_like']\n",
    "features_train['tweet_retweet_count'] = X_train['tweet_retweet_count']\n",
    "features_train['tweet_length_word'] = X_train['tweet_length_word']\n",
    "features_train['tweet_length_characters'] = X_train['tweet_length_characters']\n",
    "features_train['tweet_num_hashtags'] = X_train['tweet_num_hashtags']\n",
    "features_train['tweet_num_mention'] = X_train['tweet_num_mention']\n",
    "features_train['tweet_num_urls'] = X_train['tweet_num_urls']\n",
    "features_train['tweet_num_emoji'] = X_train['tweet_num_emoji']\n",
    "features_train['tweet_num_punctuation'] = X_train['tweet_num_punctuation']\n",
    "features_train['person_names'] = X_train['person_names']\n",
    "features_train['organize_names'] = X_train['organize_names']\n",
    "features_train['swear_words'] = X_train['swear_words']\n",
    "\n",
    "X_train_new = pd.concat([dic_all_train , features_train], axis=1)\n",
    "X_train_new = X_train_new.fillna(0)\n",
    "y_train_new = X_train['neg'].to_list()\n",
    "scaler_train = Normalizer()\n",
    "scaler_train.fit(X_train_new)\n",
    "X_train_new = scaler_train.transform(X_train_new)\n",
    "X_train_new.shape, len(y_train_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91db43",
   "metadata": {},
   "source": [
    "## Textual Features for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "980a5857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 850)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_Top = [x[0] for x in l1[:400]] + [x[0] for x in l2[:100]] + [x[0] for x in l3[:50]]\n",
    "L = X_test['full_text'].to_list()\n",
    "top_dic = {}\n",
    "for i in range(len(dic_Top)):\n",
    "    k = []\n",
    "    for j in range(len(L)):\n",
    "        if(dic_Top[i] in L[j]):\n",
    "            k.append(1)\n",
    "        else:\n",
    "            k.append(0)\n",
    "    top_dic[dic_Top[i]] = k\n",
    "df_top_s = pd.DataFrame(top_dic)\n",
    "vecs_n_test = pd.DataFrame(vecs_n_test)\n",
    "df_top_s = df_top_s.reset_index()\n",
    "vecs_n_test = vecs_n_test.reset_index()\n",
    "dic_all_test = pd.concat([vecs_n_test, df_top_s], axis=1)\n",
    "dic_all_test = dic_all_test.drop({'index'}, axis=1)\n",
    "dic_all_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9b90281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 208, 208, 208, 208, 208)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_Top0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0):\n",
    "                count += 1\n",
    "        feature_Top0_t.append(count)\n",
    "    else:\n",
    "        feature_Top0_t.append(0)\n",
    "\n",
    "feature_Top1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1):\n",
    "                count += 1\n",
    "        feature_Top1_t.append(count)\n",
    "    else:\n",
    "        feature_Top1_t.append(0)\n",
    "\n",
    "feature_Top01_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top01):\n",
    "                count += 1\n",
    "        feature_Top01_t.append(count)\n",
    "    else:\n",
    "        feature_Top01_t.append(0)\n",
    "        \n",
    "feature_Top10_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top10):\n",
    "                count += 1\n",
    "        feature_Top10_t.append(count)\n",
    "    else:\n",
    "        feature_Top10_t.append(0)\n",
    "        \n",
    "feature_weight1_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top1_w):\n",
    "                count += sort_dic1[tweet[i]]\n",
    "        feature_weight1_t.append(count)\n",
    "    else:\n",
    "        feature_weight1_t.append(0)\n",
    "        \n",
    "feature_weight0_t = []\n",
    "for i in range(len(X_test['full_text'])):\n",
    "    cv = CountVectorizer(ngram_range= (1,3))\n",
    "    if len(X_test['full_text'].iloc[i]) > 0:\n",
    "        cv.fit_transform([X_test['full_text'].iloc[i]])\n",
    "        tweet = cv.get_feature_names()\n",
    "        count = 0\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in Top0_w):\n",
    "                count += sort_dic0[tweet[i]]\n",
    "        feature_weight0_t.append(count)\n",
    "    else:\n",
    "        feature_weight0_t.append(0)\n",
    "        \n",
    "len(feature_Top0_t), len(feature_Top1_t), len(feature_Top01_t), len(feature_Top10_t), len(feature_weight1_t), len(feature_weight0_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "54d08b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1688: FutureWarning:\n",
      "\n",
      "Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(208, 868)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = pd.DataFrame(\n",
    "    {'50Top_0': feature_Top0_t,\n",
    "     '50Top_1': feature_Top1_t,\n",
    "     'Top_0_1': feature_Top01_t,\n",
    "     'Top_1_0': feature_Top10_t,\n",
    "     'Weight_0': feature_weight0_t,\n",
    "     'Weight_1': feature_weight1_t\n",
    "      })\n",
    "features_test['tweet_like'] = X_test['tweet_like']\n",
    "features_test['tweet_retweet_count'] = X_test['tweet_retweet_count']\n",
    "features_test['tweet_length_word'] = X_test['tweet_length_word']\n",
    "features_test['tweet_length_characters'] = X_test['tweet_length_characters']\n",
    "features_test['tweet_num_hashtags'] = X_test['tweet_num_hashtags']\n",
    "features_test['tweet_num_mention'] = X_test['tweet_num_mention']\n",
    "features_test['tweet_num_urls'] = X_test['tweet_num_urls']\n",
    "features_test['tweet_num_emoji'] = X_test['tweet_num_emoji']\n",
    "features_test['tweet_num_punctuation'] = X_test['tweet_num_punctuation']\n",
    "features_test['person_names'] = X_test['person_names']\n",
    "features_test['organize_names'] = X_test['organize_names']\n",
    "features_test['swear_words'] = X_test['swear_words']\n",
    "\n",
    "X_test_new = pd.concat([dic_all_test , features_test], axis=1)\n",
    "X_test_new = X_test_new.fillna(0)\n",
    "scaler_test = Normalizer()\n",
    "scaler_test.fit(X_test_new)\n",
    "X_test_new = scaler_test.transform(X_test_new)\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45f8bf",
   "metadata": {},
   "source": [
    "## Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ad067",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "'''parameter_space = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.05, 0.1],\n",
    "    'learning_rate': ['constant','adaptive']}'''\n",
    "\n",
    "clf1 = MLPClassifier(random_state=0, max_iter=400)\n",
    "#clf1 = GridSearchCV(mlp, parameter_space, cv=5, n_jobs=1)\n",
    "clf1.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('****MLP classificaion report for test set****')\n",
    "predictions1 = clf1.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions1))\n",
    "print(metrics.roc_auc_score(y_test,predictions1))\n",
    "\n",
    "print('\\n****MLP classificaion report for train set****')\n",
    "predictions1_1 = clf1.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_1))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_1))\n",
    "\n",
    "#XGB\n",
    "'''parameter_space={'max_depth': range(3, 18),\n",
    "                    'gamma': range(1,9)}'''\n",
    "\n",
    "eval_set = [(X_test_new, y_test)]\n",
    "clf2 = XGBClassifier(random_state=0, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set, verbose=True)\n",
    "#clf2 = GridSearchCV(xgb, parameter_space, cv=3)\n",
    "clf2.fit(X_train_new, y_train_new, verbose=True)\n",
    "\n",
    "print('\\n\\n\\n\\n****XGB classificaion report for test set****')\n",
    "predictions2 = clf2.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions2))\n",
    "print(metrics.roc_auc_score(y_test,predictions2))\n",
    "\n",
    "print('\\n****XGB classificaion report for train set****')\n",
    "predictions1_2 = clf2.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_2))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_2))\n",
    "\n",
    "#RF\n",
    "'''parameter_space={'max_depth' : list(range(1, 50))}'''\n",
    "\n",
    "clf3 = RandomForestClassifier(random_state=0)\n",
    "#clf3 = GridSearchCV(rf, parameter_space, cv=3, scoring=p_score)\n",
    "clf3.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****RF classificaion report for test set****')\n",
    "predictions3 = clf3.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions3))\n",
    "print(metrics.roc_auc_score(y_test,predictions3))\n",
    "\n",
    "print('\\n****RF classificaion report for train set****')\n",
    "predictions1_3 = clf3.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_3))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_3))\n",
    "\n",
    "#LR\n",
    "'''parameter_space={'solver' : ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "                'penalty' : ['l2'],\n",
    "                'C' : [100, 10, 1.0, 0.1, 0.01]}'''\n",
    "\n",
    "clf4 = LogisticRegression(random_state=0)\n",
    "#clf4 = GridSearchCV(lr, parameter_space, cv=3, scoring=p_score)\n",
    "clf4.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****LR classificaion report for test set****')\n",
    "predictions4 = clf4.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions4))\n",
    "print(metrics.roc_auc_score(y_test,predictions4))\n",
    "\n",
    "print('\\n****LR classificaion report for train set****')\n",
    "predictions1_4 = clf4.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_4))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_4))\n",
    "\n",
    "#NB\n",
    "clf5 = GaussianNB()\n",
    "clf5.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****NB classificaion report for test set****')\n",
    "predictions5 = clf5.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions5))\n",
    "print(metrics.roc_auc_score(y_test,predictions5))\n",
    "\n",
    "print('\\n****NB classificaion report for train set****')\n",
    "predictions1_5 = clf5.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_5))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_5))\n",
    "\n",
    "#SVM\n",
    "clf6 = svm.SVC(random_state=0)\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SVM classificaion report for test set****')\n",
    "predictions6 = clf6.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions6))\n",
    "print(metrics.roc_auc_score(y_test,predictions6))\n",
    "\n",
    "print('\\n****SVM classificaion report for train set****')\n",
    "predictions1_6 = clf6.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_6))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_6))\n",
    "\n",
    "#SGD\n",
    "clf7 = SGDClassifier(max_iter=2000, tol=1e-3, random_state=0)\n",
    "clf7.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****SGD classificaion report for test set****')\n",
    "predictions7 = clf7.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions7))\n",
    "print(metrics.roc_auc_score(y_test,predictions7))\n",
    "\n",
    "print('\\n****SGD classificaion report for train set****')\n",
    "predictions1_7 = clf7.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_7))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_7))\n",
    "\n",
    "#KNN\n",
    "'''parameter_space = {'n_neighbors' : list(range(1,300))}'''\n",
    "clf8 = KNeighborsClassifier()\n",
    "#clf8 = GridSearchCV(kn, parameter_space, n_jobs=-1, cv=5, scoring='f1')\n",
    "clf8.fit(X_train_new, y_train_new)\n",
    "\n",
    "print('\\n\\n\\n\\n****KNN classificaion report for test set****')\n",
    "predictions8 = clf8.predict(X_test_new)\n",
    "print(metrics.classification_report(y_test,predictions8))\n",
    "print(metrics.roc_auc_score(y_test,predictions8))\n",
    "\n",
    "print('\\n****KNN classificaion report for train set****')\n",
    "predictions1_8 = clf8.predict(X_train_new)\n",
    "print(metrics.classification_report(y_train_new,predictions1_8))\n",
    "print(metrics.roc_auc_score(y_train_new,predictions1_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b1b5b8",
   "metadata": {},
   "source": [
    "## Feature Importance Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0a4d0b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#feature importances for MLP\n",
    "l1 = []\n",
    "l2 = clf1.coefs_[0]\n",
    "for i in l2:\n",
    "    l1.append(sum(i))\n",
    "importance_1 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_1.append(i)\n",
    "importance_1 = [(float(i)-min(importance_1))/(max(importance_1)-min(importance_1)) for i in importance_1]\n",
    "\n",
    "#feature importances for XGB\n",
    "l1 = []\n",
    "l2 = clf2.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_2 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_2.append(i)\n",
    "importance_2\n",
    "importance_2 = [(float(i)-min(importance_2))/(max(importance_2)-min(importance_2)) for i in importance_2]\n",
    "\n",
    "#feature importances for RF\n",
    "l1 = []\n",
    "l2 = clf3.feature_importances_\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_3 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_3.append(i)\n",
    "importance_3 = [(float(i)-min(importance_3))/(max(importance_3)-min(importance_3)) for i in importance_3]\n",
    "\n",
    "#feature importances for LR\n",
    "l1 = []\n",
    "l2 = clf4.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_4 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_4.append(i)\n",
    "importance_4 = [(float(i)-min(importance_4))/(max(importance_4)-min(importance_4)) for i in importance_4]\n",
    "\n",
    "#feature importances for NB\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf5, X_test_new, y_test)\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_5 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_5.append(i)\n",
    "importance_5 = [(float(i)-min(importance_5))/(max(importance_5)-min(importance_5)) for i in importance_5]\n",
    "\n",
    "#feature importances for SVM\n",
    "l1 = []\n",
    "clf6 = svm.SVC(random_state=0, kernel='linear')\n",
    "clf6.fit(X_train_new, y_train_new)\n",
    "l2 = clf6.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_6 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_6.append(i)\n",
    "importance_6 = [(float(i)-min(importance_6))/(max(importance_6)-min(importance_6)) for i in importance_6]\n",
    "\n",
    "#feature importances for SGD\n",
    "l1 = []\n",
    "l2 = clf7.coef_[0]\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_7 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_7.append(i)\n",
    "importance_7 = [(float(i)-min(importance_7))/(max(importance_7)-min(importance_7)) for i in importance_7]\n",
    "\n",
    "#feature importances for KNN\n",
    "l1 = []\n",
    "l2 = permutation_importance(clf, X_test_new, y_test, scoring='neg_mean_squared_error')\n",
    "l2 = l2.importances_mean\n",
    "for i in l2:\n",
    "    l1.append(i)\n",
    "importance_8 = [sum(l1[:300]) , sum(l1[300:700]), sum(l1[700:800]) , sum(l1[800:850])]\n",
    "for i in l1[850:]:\n",
    "    importance_8.append(i)\n",
    "importance_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "886d8674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "col_name = ['embedding','unigram','bigram','trigram','Top_0','Top_1','Top_0_1','Top_1_0','Weight_0',\n",
    "            'Weight_1','like','retweet','len_word','len_chars','hashtags','mention','urls','emoji',\n",
    "            'punctuation','person','organize','swear']\n",
    "N = 12\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[10:]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[10:]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[10:]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[10:]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[10:]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[10:]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[10:]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[10:]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[10:],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N)\n",
    "width = 0.1\n",
    "\n",
    "xvals = importance_1[:10]\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'r')\n",
    "  \n",
    "yvals = importance_2[:10]\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='g')\n",
    "\n",
    "zvals = importance_3[:10]\n",
    "bar3 = plt.bar(ind+width*2, zvals, width, color = 'b')\n",
    "\n",
    "z1vals = importance_4[:10]\n",
    "bar4 = plt.bar(ind+width*3, z1vals, width, color = 'c')\n",
    "\n",
    "z2vals = importance_5[:10]\n",
    "bar5 = plt.bar(ind+width*4, z2vals, width, color = 'm')\n",
    "\n",
    "z3vals = importance_6[:10]\n",
    "bar6 = plt.bar(ind+width*5, z3vals, width, color = 'y')\n",
    "\n",
    "z4vals = importance_7[:10]\n",
    "bar7 = plt.bar(ind+width*6, z4vals, width, color = 'lime')\n",
    "\n",
    "z5vals = importance_8[:10]\n",
    "bar8 = plt.bar(ind+width*7, z5vals, width, color = 'violet')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(23, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=15)\n",
    "plt.title(\"Textual Feature Importance for Negativity Detection Algorithms(for 2382 tweets with underampling)\",fontsize=15)\n",
    "plt.xticks(ind+width,col_name[:10],fontsize=15)\n",
    "plt.legend((bar1, bar2, bar3 , bar4, bar5, bar6, bar7, bar8), ('MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_2.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7e3a5",
   "metadata": {},
   "source": [
    "## Overfitting Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3eed2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_f1_test_0 = [0.69, 0.67, 0.77, 0.63, 0.56, 0.65, 0.58, 0.70, 0.76]\n",
    "list_f1_train_0 = [0.90, 1, 1, 0.64, 0.87, 0.65, 0.61, 0.77, 0.89]\n",
    "list_f1_test_1 = [0.53, 0.49, 0.52, 0.51, 0.50, 0.52 ,0.51, 0.50, 0.63]\n",
    "list_f1_train_1 = [0.91, 1, 1, 0.74, 0.89, 0.74, 0.74, 0.79, 0.89]\n",
    "N = 9\n",
    "ind = np.arange(N)\n",
    "width = 0.3\n",
    "\n",
    "xvals = list_f1_test_0\n",
    "bar1 = plt.bar(ind, xvals, width, color = 'lightcoral')\n",
    "  \n",
    "yvals = list_f1_train_0\n",
    "bar2 = plt.bar(ind+width, yvals, width, color='brown')\n",
    "\n",
    "fig.set_size_inches(20, 5.5)\n",
    "plt.ylabel('Imortance Scores',fontsize=12)\n",
    "plt.title(\"F1-Score for Class 0 in Negativity Detection(for 2382 tweets)\",fontsize=12)\n",
    "plt.xticks(ind+width,['MLP' , 'XGB' , 'RF' , 'LR' , 'NB' , 'SVM' , 'SGD' , 'KNN' , 'P-BERT'], fontsize=10)\n",
    "plt.legend((bar1, bar2), ('test' , 'train'),fontsize=12, bbox_to_anchor=(1, 1))\n",
    "#plt.savefig(r'C:/PRIVATE/Metodata/Metodata-Files/Negativity_Files/neg_under_overfit.jpg' ,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55944d4",
   "metadata": {},
   "source": [
    "## Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e593c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_excel(r'../Files/5100_new.xlsx')\n",
    "data['full_text'] = data1['full_text']\n",
    "data['rate'] = data1['neg']\n",
    "\n",
    "data['rate'] = data['rate'].apply(lambda r: r if r < 2 else None)\n",
    "\n",
    "data = data.dropna(subset=['rate'])\n",
    "data = data.dropna(subset=['full_text'])\n",
    "data = data.drop_duplicates(subset=['full_text'], keep='first')\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "print('data information')\n",
    "print(data.info(), '\\n')\n",
    "\n",
    "# print missing values information\n",
    "print('missing values stats')\n",
    "print(data.isnull().sum(), '\\n')\n",
    "\n",
    "# print some missing values\n",
    "print('some missing values')\n",
    "print(data[data['rate'].isnull()].iloc[:5], '\\n')\n",
    "\n",
    "data['comment_len_by_words'] = data['full_text'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "min_max_len = data[\"comment_len_by_words\"].min(), data[\"comment_len_by_words\"].max()\n",
    "print(f'Min: {min_max_len[0]} \\tMax: {min_max_len[1]}')\n",
    "\n",
    "minlim, maxlim = 0, 47\n",
    "\n",
    "data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n",
    "data = data.dropna(subset=['comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "unique_rates = list(sorted(data['rate'].unique()))\n",
    "\n",
    "def rate_to_label(rate, threshold=0.0):\n",
    "    if rate <= threshold:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "data['label'] = data['rate'].apply(lambda t: rate_to_label(t, 0.0))\n",
    "labels = list(sorted(data['label'].unique()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea330db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "  \n",
    "    text = text.strip()\n",
    "    \n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "        fix_unicode=True,\n",
    "        to_ascii=False,\n",
    "        lower=True,\n",
    "        no_line_breaks=True,\n",
    "        no_urls=True,\n",
    "        no_emails=True,\n",
    "        no_phone_numbers=True,\n",
    "        no_numbers=False,\n",
    "        no_digits=False,\n",
    "        no_currency_symbols=True,\n",
    "        no_punct=False,\n",
    "        replace_with_url=\"\",\n",
    "        replace_with_email=\"\",\n",
    "        replace_with_phone_number=\"\",\n",
    "        replace_with_number=\"\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "    \n",
    "    # normalizing\n",
    "    normalizer = hazm.Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u'\\U00010000-\\U0010ffff'\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\u3030\"\n",
    "        u\"\\ufe0f\"\n",
    "        u\"\\u2069\"\n",
    "        u\"\\u2066\"\n",
    "        # u\"\\u200c\"\n",
    "        u\"\\u2068\"\n",
    "        u\"\\u2067\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "    \n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c134cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning comments\n",
    "data['cleaned_comment'] = data['full_text'].apply(cleaning)\n",
    "\n",
    "#calculate the length of comments based on their words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))\n",
    "\n",
    "#remove comments with the length of fewer than three words\n",
    "data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n",
    "data = data.dropna(subset=['cleaned_comment_len_by_words'])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data = data[['full_text', 'label']]\n",
    "data.columns = ['comment', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c60652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].apply(lambda t: 1-labels.index(t))\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0, stratify=data['label'])\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=0, stratify=train['label'])\n",
    "negative_data = train[train['label'] == 'negative']\n",
    "positive_data = train[train['label'] == 'positive']\n",
    "\n",
    "cutting_point = min(len(negative_data), len(positive_data))\n",
    "\n",
    "if cutting_point <= len(negative_data):\n",
    "    negative_data = negative_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "if cutting_point <= len(positive_data):\n",
    "    positive_data = positive_data.sample(n=cutting_point).reset_index(drop=True)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "if len(positive_data) > len(negative_data):\n",
    "    negative_data = negative_data.sample(n=len(positive_data), replace=True).reset_index(drop=True)\n",
    "\n",
    "if len(positive_data) < len(negative_data):\n",
    "    positive_data = positive_data.sample(n=len(negative_data), replace=True).reset_index(drop=True)'''\n",
    "train = pd.concat([negative_data, positive_data])\n",
    "#new_data = new_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()\n",
    "x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()\n",
    "x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import TFBertModel, TFBertForSequenceClassification\n",
    "from transformers import glue_convert_examples_to_features\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general config\n",
    "MAX_LEN = 79\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 4\n",
    "EEVERY_EPOCH = 500\n",
    "LEARNING_RATE = 2e-5\n",
    "CLIP = 0.0\n",
    "dropout_rate = 0.4\n",
    "\n",
    "MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-uncased'\n",
    "#MODEL_NAME_OR_PATH = 'bert-base-multilingual-cased'\n",
    "#MODEL_NAME_OR_PATH = 'distilbert-base-multilingual-cased'\n",
    "OUTPUT_PATH = r'./pytorch_model.bin'\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: 1-i for i, label in enumerate(labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "config = BertConfig.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, **{\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "    })\n",
    "config.hidden_dropout_prob = 0.5\n",
    "config.attention_probs_dropout_prob = 0.5\n",
    "print(config.to_json_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27cd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample:\n",
    "    \"\"\" A single example for simple sequence classification. \"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\" Constructs a InputExample. \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=\"classification\", is_tf_dataset=True):\n",
    "    examples = []\n",
    "    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)\n",
    "\n",
    "    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):\n",
    "        guid = \"%s\" % i\n",
    "        label = int(_y)\n",
    "        \n",
    "        if isinstance(_x, str):\n",
    "            text_a = _x\n",
    "            text_b = None\n",
    "        else:\n",
    "            assert len(_x) == 2\n",
    "            text_a = _x[0]\n",
    "            text_b = _x[1]\n",
    "        \n",
    "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
    "    \n",
    "    features = glue_convert_examples_to_features(\n",
    "        examples, \n",
    "        tokenizer, \n",
    "        maxlen, \n",
    "        output_mode=output_mode, \n",
    "        label_list=list(np.unique(y)))\n",
    "\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "    all_token_type_ids = []\n",
    "    all_labels = []\n",
    "\n",
    "    for f in tqdm(features, position=0, total=len(examples)):\n",
    "        if is_tf_dataset:\n",
    "            all_input_ids.append(tf.constant(f.input_ids))\n",
    "            all_attention_masks.append(tf.constant(f.attention_mask))\n",
    "            all_token_type_ids.append(tf.constant(f.token_type_ids))\n",
    "            all_labels.append(tf.constant(f.label))\n",
    "        else:\n",
    "            all_input_ids.append(f.input_ids)\n",
    "            all_attention_masks.append(f.attention_mask)\n",
    "            all_token_type_ids.append(f.token_type_ids)\n",
    "            all_labels.append(f.label)\n",
    "\n",
    "    if is_tf_dataset:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'token_type_ids': all_token_type_ids\n",
    "        }, all_labels))\n",
    "\n",
    "        return dataset, features\n",
    "    \n",
    "    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]\n",
    "    ydata = all_labels\n",
    "\n",
    "    return [xdata, ydata], features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=256)\n",
    "valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=256)\n",
    "\n",
    "test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256)\n",
    "[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=256, is_tf_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_dataset(dataset, batch_size):\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset, batch_size):\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)\n",
    "valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)\n",
    "\n",
    "train_steps = len(train_examples) // TRAIN_BATCH_SIZE\n",
    "valid_steps = len(valid_examples) // VALID_BATCH_SIZE\n",
    "\n",
    "train_steps, valid_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fd047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "def build_model(model_name, config, learning_rate=3e-5):\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(MODEL_NAME_OR_PATH, config, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54dc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    steps_per_epoch=train_steps,\n",
    "    validation_steps=valid_steps,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "final_accuracy = r.history['val_accuracy']\n",
    "print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))\n",
    "\n",
    "model.save_pretrained(os.path.dirname(OUTPUT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d326397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))\n",
    "print()\n",
    "print(f'Evaluation: {ev}')\n",
    "print()\n",
    "\n",
    "predictions = model.predict(xtest)\n",
    "ypred = predictions[0].argmax(axis=-1).tolist()\n",
    "\n",
    "print()\n",
    "print(classification_report(ytest, ypred, target_names=labels))\n",
    "print()\n",
    "\n",
    "print(f'F1: {f1_score(ytest, ypred, average=\"weighted\")}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
